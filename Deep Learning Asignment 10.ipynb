{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "What does a SavedModel contain?\n",
    "A SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "When should you use TF Serving? What are its main features?\n",
    "TF Serving is a part of TFX or TensorFlow Extended, simply enough TF Extended is an API that is designed to help you to make production-ready, Machine Learning systems. We will only be talking about a specific subpart of TFX called TF serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "How do you deploy a model across multiple TF Serving instances?\n",
    "TF Model server helps you in doing so very easily. A really wonderful part about TF Model Server is to let you focus on writing real code and not worry about infrastructure and managing it and that is something really useful, you do not want to spend time doing these infrastructure things as a developer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "When should you use the gRPC API rather than the REST API to query a model served by TF\n",
    "Serving?\n",
    "The RESTful APIs support a canonical encoding in JSON, making it easier to share data between systems. For supported types, the encodings are described on a type-by-type basis in the table below. Types not listed below are implied to be unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile\n",
    "Remember quantization may come with the cost of reducing the accuracy of your model. Then, in case of no degradation, it is fine to simply convert your model to TFLite model which can be then imported and interpreted by the TFLite interpreter running on the device itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd40ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is quantization-aware training, and why would you need it?\n",
    "Quantization-aware training allows you to do this (TensorFlow, n.d.), by emulating inference-time quantization during the fitting process. Doing so allows your model to learm parameters that are robust against the loss of precision invoked with quantization (Tfmot.quantization.keras.quantize_model, n.d.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are model parallelism and data parallelism?\n",
    "Data parallelism is when you use the same model for every thread, but feed it with different parts of the data; model parallelism is when you use the same data for every thread, but split the model among threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.distribute.Strategyis a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes. tf.distribute.Strategyhas been designed with these key goals in mind: 1. Easy to use and support multiple user segments, including researchers, machine learning engineers, etc. 2. Provide good performance out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e50ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fbef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
