{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write the Python code to implement a single neuron.\n",
    "# Python program to implement a\n",
    "# single neuron neural network\n",
    "\n",
    "# import all necessary libraries\n",
    "from numpy import exp, array, random, dot, tanh\n",
    "\n",
    "# Class to create a neural\n",
    "# network with single neuron\n",
    "class NeuralNetwork():\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\t# Using seed to make sure it'll\n",
    "\t\t# generate same weights in every run\n",
    "\t\trandom.seed(1)\n",
    "\t\t\n",
    "\t\t# 3x1 Weight matrix\n",
    "\t\tself.weight_matrix = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "\t# tanh as activation function\n",
    "\tdef tanh(self, x):\n",
    "\t\treturn tanh(x)\n",
    "\n",
    "\t# derivative of tanh function.\n",
    "\t# Needed to calculate the gradients.\n",
    "\tdef tanh_derivative(self, x):\n",
    "\t\treturn 1.0 - tanh(x) ** 2\n",
    "\n",
    "\t# forward propagation\n",
    "\tdef forward_propagation(self, inputs):\n",
    "\t\treturn self.tanh(dot(inputs, self.weight_matrix))\n",
    "\t\n",
    "\t# training the neural network.\n",
    "\tdef train(self, train_inputs, train_outputs,\n",
    "\t\t\t\t\t\t\tnum_train_iterations):\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t# Number of iterations we want to\n",
    "\t\t# perform for this set of input.\n",
    "\t\tfor iteration in range(num_train_iterations):\n",
    "\t\t\toutput = self.forward_propagation(train_inputs)\n",
    "\n",
    "\t\t\t# Calculate the error in the output.\n",
    "\t\t\terror = train_outputs - output\n",
    "\n",
    "\t\t\t# multiply the error by input and then\n",
    "\t\t\t# by gradient of tanh function to calculate\n",
    "\t\t\t# the adjustment needs to be made in weights\n",
    "\t\t\tadjustment = dot(train_inputs.T, error *\n",
    "\t\t\t\t\t\t\tself.tanh_derivative(output))\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t# Adjust the weight matrix\n",
    "\t\t\tself.weight_matrix += adjustment\n",
    "\n",
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\tneural_network = NeuralNetwork()\n",
    "\t\n",
    "\tprint ('Random weights at the start of training')\n",
    "\tprint (neural_network.weight_matrix)\n",
    "\n",
    "\ttrain_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "\ttrain_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "\tneural_network.train(train_inputs, train_outputs, 10000)\n",
    "\n",
    "\tprint ('New weights after training')\n",
    "\tprint (neural_network.weight_matrix)\n",
    "\n",
    "\t# Test the neural network with a new situation.\n",
    "\tprint (\"Testing network on new examples ->\")\n",
    "\tprint (neural_network.forward_propagation(array([1, 0, 0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53dc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write the Python code to implement ReLU.\n",
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    " \n",
    "x = 1.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -10.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 0.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 15.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -20.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "# Program to multiply two matrices using nested loops\n",
    " \n",
    "# take a 3x3 matrix\n",
    "A = [[12, 7, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]]\n",
    " \n",
    "# take a 3x4 matrix   \n",
    "B = [[5, 8, 1, 2],\n",
    "    [6, 7, 3, 0],\n",
    "    [4, 5, 9, 1]]\n",
    "     \n",
    "result = [[0, 0, 0, 0],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 0, 0]]\n",
    " \n",
    "# iterating by row of A\n",
    "for i in range(len(A)):\n",
    " \n",
    "    # iterating by column by B\n",
    "    for j in range(len(B[0])):\n",
    " \n",
    "        # iterating by rows of B\n",
    "        for k in range(len(B)):\n",
    "            result[i][j] += A[i][k] * B[k][j]\n",
    " \n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6547c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the “hidden size” of a layer?\n",
    "Hidden size is number of features of the hidden state for RNN. So if you increase hidden size then you compute bigger feature as hidden state output. However, num_layers is just multiple RNN units which contain hidden states with given hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "What does the t method do in PyTorch?\n",
    "\n",
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab (FAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix multiplications in NumPy are reasonably fast without the need for optimization. However, if every second counts, it is possible to significantly improve performance (even without a GPU).\n",
    "\n",
    "Below are a collection of small tricks that can help with large (~4000x4000) matrix multiplications. I have used them to reduce inference time in a deep neural network from 24 seconds to less than one second. In fact, in one case, my optimized code on a CPU turned out to run faster than Tensorflow using a GPU (1 second vs 7 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee110f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Do not forget that cell magic starts with %% and line magic starts with %. An easier way is to use ExecuteTime plugin in jupyter_contrib_nbextensions package. You can use timeit magic function for that. I simply added %%time at the beginning of the cell and got the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1098fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is elementwise arithmetic?\n",
    "Each pair of elements in corresponding locations are added together to produce a new tensor of the same shape. So, addition is an element-wise operation, and in fact, all the arithmetic operations, add, subtract, multiply, and divide are element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    ">> my_list1 = [30, 34, 56]\n",
    ">>> my_list2 = [29, 500, 43]\n",
    ">>> all(i >= 30 for i in my_list1)\n",
    "True\n",
    ">>> all(i >= 30 for i in my_list2).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e653f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank 0 Tensor: The familiar scalar is the simplest tensor and is a rank 0 tensor. Scalars are just single real numbers like ½, 99 or -1002 that are used to measure magnitude (size). Scalars can technically be written as a one-unit array: [½], or [-1002], but it’s not usual practice to do so. Rank 1 Tensor: Vectors are rank 1 tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "With elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the i -th row of a and the j -th column of b before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded...\n",
    "Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that...\n",
    "Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7cd689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
