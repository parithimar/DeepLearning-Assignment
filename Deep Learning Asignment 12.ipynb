{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e26029",
   "metadata": {},
   "outputs": [],
   "source": [
    "How does unsqueeze help us to solve certain broadcasting problems?\n",
    "There are cases where broadcasting is a bad idea because it leads to inefficient use of memory that slow down the computation. Let’s assume that we have a large data set, each datum is a list of parameters. In Numpy we have a 2-D array, where each row is a datum and the number of rows is the size of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a86468",
   "metadata": {},
   "outputs": [],
   "source": [
    "How can we use indexing to do the same operation as unsqueeze?\n",
    "Indexes are meant to speed up the performance of a database, so use indexing whenever it significantly improves the performance of your database. As your database becomes larger and larger, the more likely you are to see benefits from indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "How do we show the actual contents of the memory used for a tensor?\n",
    "Let's create some basic tensors. Here is a \"scalar\" or \"rank-0\" tensor . A scalar contains a single value, and no \"axes\". A \"vector\" or \"rank-1\" tensor is like a list of values. A vector has one axis: A \"matrix\" or \"rank-2\" tensor has two axes: Tensors may have more axes; here is a tensor with three axes: There are many ways you might visualize a tensor with more than two axes. You can convert a tensor to a NumPy array either using np.array or the tensor.numpymethod: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9233029",
   "metadata": {},
   "outputs": [],
   "source": [
    "When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix?\n",
    "Matrix addition can only be performed on matrices of the same size. This means that you can only add matrices if both matrices are m × n. For example, you can add two or more 3 × 3, 1 × 2, or 5 × 4 matrices. You cannot add a 2 × 3 and a 3 × 2 matrix, a 4 × 4 and a 3 × 3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "When the broadcasted relation is small enough, broadcast joins are fast, as they require minimal data shuffling. Above a certain threshold however, broadcast joins tend to be less reliable or performant than shuffle-based join algorithms, due to bottlenecks in network and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implement matmul using Einstein summation.\n",
    "import torch\n",
    "from time import time\n",
    "torch.cuda.synchronize()\n",
    "vect_a = torch.rand(10)\n",
    "vect_b = torch.rand(10)\n",
    "# Dot product\n",
    "# method 1: matmul\n",
    "start1 = time()\n",
    "inner_ab1 = torch.matmul(vect_a, vect_b)\n",
    "elapsed_time1 = (time()-start1)\n",
    "print(\"Elapsed time for matmul [ms]: \", elapsed_time1*1000)\n",
    "# method 2: einstein sum\n",
    "start2 = time()\n",
    "inner_ab2 = torch.einsum('i,i->', vect_a, vect_b)\n",
    "elapsed_time2 = (time()-start2)\n",
    "print(\"Elapsed time for the Einstein sum [ms]: \", elapsed_time2*1000)\n",
    "# compute L2 norm based error between the two results\n",
    "discrepancy_L2 = (inner_ab1 - inner_ab2).norm(p=2)\n",
    "print(\"L2 error: \", discrepancy_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f473551",
   "metadata": {},
   "outputs": [],
   "source": [
    "An index that is summed over is a summation index, in this case \" i \". It is also called a dummy index since any symbol can replace \" i \" without changing the meaning of the expression provided that it does not collide with index symbols in the same term. An index that is not summed over is a free index and should appear only once per term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de413023",
   "metadata": {},
   "outputs": [],
   "source": [
    " Einstein summation only applies to very specific summations which follow four basic rules (Evans, 2020): \n",
    "\n",
    "The summation sign is omitted.\n",
    "A summation is implied if the index appears twice. For example, A i B i = A 1 sub>B 1 + A 2 B 2 + A 3 B 3, where i is a...\n",
    "A suffix appearing just once can take any value. For example, A i = B i holds for i = 1, 2, 3. (there can be multiple...\n",
    "A suffix can’t appear more than two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forward pass is a technique to move forward through network diagram to determining project duration and finding the critical path or Free Float of the project. Whereas backward pass represents moving backward to the end result to calculate late start or to find if there is any slack in the activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "To move forward through the network, called a forward pass, we iteratively use a formula to calculate each neuron in the next layer. Keep a total disregard for the notation here, but we call neurons for activations $a$, weights $w$ and biases $b$ — which is cumulated in vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fe0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "The reason the validation loss is more stable is that it is a continuous function: It can distinguish that prediction 0.9 for a positive sample is more correct than a prediction 0.51. For accuracy, you round these continuous logit predictions to { 0; 1 } and simply compute the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "All in all, initializing weights with inappropriate values will lead to divergence or a slow-down in the training of your neural network. Although we illustrated the exploding/vanishing gradient problem with simple symmetrical weight matrices, the observation generalizes to any initialization values that are too small or too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66950fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
