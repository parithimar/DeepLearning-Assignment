{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "The sigmoid activation function, also called the logistic function, is traditionally a very popular activation function for neural networks. The input to the function is transformed into a value between 0.0 and 1.0. Inputs that are much larger than 1.0 are transformed to the value 1.0, similarly, values much smaller than 0.0 are snapped to 0.0.\n",
    "\n",
    "Tanh function\n",
    "Tanh function is similar to the sigmoid function but this step function is symmetric around the origin (zero centric function). Tanh function ranges from -1 to 1\n",
    "ReLU (Rectified Linear Unit)\n",
    "ReLU activation function activates neurons only if the input of the step function is more than 0; otherwise deactivates.\n",
    "\n",
    "Unlike ReLU, Leaky ReLU allows a small constant slope for negative inputs, enabling backpropagation, even for negative input values\n",
    "\n",
    "SWISH\n",
    "The swish function was inspired by the sigmoid function. This function is used for gating in LSTMs and highway networks. We use the same value for gating to simplify the gating mechanism, which is called self-gating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f83cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "What happens when you increase or decrease the optimizer learning rate?\n",
    "When entering the optimal learning rate zone, you'll observe a quick drop in the loss function. Increasing the learning rate further will cause an increase in the loss as the parameter updates cause the loss to \"bounce around\" and even diverge from the minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b72967",
   "metadata": {},
   "outputs": [],
   "source": [
    "What happens when you increase the number of internal hidden neurons?\n",
    "An inordinately large number of neurons in the hidden layers can increase the time it takes to train the network. The amount of training time can increase to the point that it is impossible to adequately train the neural network. Obviously, some compromise must be reached between too many and too few neurons in the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbde732",
   "metadata": {},
   "outputs": [],
   "source": [
    "What happens when you increase the size of batch computation?\n",
    "Practitioners often want to use a larger batch size to train their model as it allows computational speedups from the parallelism of GPUs. However, it is well known that too large of a batch size will lead to poor generalization (although currently itâ€™s not known why this is so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "This is exactly why we use it for. Regularization is a form of regression used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting. It discourages the fitting of a complex model, thus reducing the variance and chances of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Almost universally, deep learning neural networks are trained under the framework of maximum likelihood using cross-entropy as the loss function. Most modern neural networks are trained using maximum likelihood. This means that the cost function is ] described as the cross-entropy between the training data and the model distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Underfitting is on the opposite end of the spectrum. A model is said to be underfitting when it's not even able to classify the data it was trained on, let alone data it hasn't seen before. A model is said to be underfitting when it's not able to classify the data it was trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f021fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. This is called dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe816e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f116c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
