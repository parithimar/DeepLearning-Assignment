{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "No. All weights should be initialized to different random values and should not have the same initial value. If weights are symmetrical, meaning they have the same value, it makes it almost impossible for backpropagation to converge to a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e75533",
   "metadata": {},
   "outputs": [],
   "source": [
    "It can take on negative values, so the average output of the neurons in any given layer is typically closer to 0 then when using the ReLU function. This helps alleviate the vanishing gradients problem. The vanishing gradients problem is the idea that gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the gradient descent update leaves the lower layer connection weights virtually unshanged, and training never converges to a good solution.\n",
    "It always has a non-zero derivative, which avoids the dying units issue that can affect ReLU units. \"Dying ReLU's\" is the problem where units stop outputting anything other than 0. In some cases, you may find that half of your networks neurons are dead, especially if you use a large learning rate.\n",
    "It is smooth everywhere, which helps gradient descent converge faster. ReLU's slope abruptly jumps from 0 to 1 at z = 0. Such an abrupt change can slow down gradient descent because it will bounce around z = 0.\n",
    "(For vanishing gradients see p. 275) (For ELU see p. 281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation functions are generally two types, These are\n",
    "Linear or Identity Activation Function\n",
    "Non-Linear Activation Function.\n",
    "Generally, neural networks use non-linear activation functions, which can help the network learn complex data, compute and learn almost any function representing a question, and provide accurate predictions.They allow back-propagation because they have a derivative function which is related to the inputs.\n",
    "Non-linear Activation Functions:\n",
    "Above listed all activation functions are belong to non-linear activation functions. And we will discuss below more in details.\n",
    "Sigmoid Activation Function:\n",
    "Sigmoid Activation function is very simple which takes a real value as input and gives probability that ‘s always between 0 or 1. It looks like ‘S’ shape.\n",
    "\n",
    "Sigmoid function and it’s derivative\n",
    "It’s non-linear, continuously differentiable, monotonic, and has a fixed output range. Main advantage is simple and good for classifier. But Big disadvantage of the function is that it It gives rise to a problem of “vanishing gradients” because Its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder. That takes very high computational time in hidden layer of neural network\n",
    "# sigmoid function\n",
    "def sigmoid(z):\n",
    "  return 1.0 / (1 + np.exp(-z))\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z) * (1-sigmoid(z))\n",
    "2. Tanh or Hyperbolic tangent:\n",
    "Tanh help to solve non zero centered problem of sigmoid function. Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep learning neural networks are trained using the stochastic gradient descent algorithm.\n",
    "\n",
    "Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.\n",
    "\n",
    "The amount that the weights are updated during training is referred to as the step size or the “learning rate.”\n",
    "\n",
    "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n",
    "\n",
    "The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
    "\n",
    "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.\n",
    "\n",
    "The challenge of training deep learning neural networks involves carefully selecting the learning rate. It may be the most important hyperparameter for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ef7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "A sparse model is that where most weights are equal to 0. There's a couple ways of achieving that effect.\n",
    "\n",
    "You can train the model normally then zero out tiny weights.\n",
    "\n",
    "For more sparsity, you can apply l1 regularization during training, which pushes the optimizer towards sparsity.\n",
    "\n",
    "Finally, you can combine l1 regulatization with dual averaging using TensorFlow's FTRLOptimizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout is a popular regularization technique for deep neural networks.\n",
    "\n",
    "The algorithm is: at each training step, every neuron (including the input neurons but excluding the output neurons) has a probability p of being temporariliy \"dropped out\", meaning it will be entierly ignored during this training step. However, it may be active during the next step.\n",
    "\n",
    "The hyperparameter p is called the dropout rate, and it is typically set to 0.5.\n",
    "\n",
    "After training the neurons don't get dropped anymore. That's the gist of the algorithm.\n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. Remember, because dropout is only tuned during training, it has no impact on inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc18fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122012d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b5183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0453c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
