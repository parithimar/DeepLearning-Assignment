{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e359f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "RNN can use their internal memory for processing the arbitrary series of inputs which is not the case with feedforward neural networks. Due to its recurrent nature, the computation is slow. Training of RNN models can be difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d732be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "The encoder uses the embeddings of the source sentence for its keys, values and queries, whereas the decoder uses the output of the encoder for its keys and values, and embedding of the target sentence for its query. Let’s again take a sentence “I like cats more than dogs”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "How can you deal with variable-length input sequences?\n",
    "The standard way of working with inputs of variable lengths is to pad all the sequences with zeros to make their lengths equal to the length of the largest sequence. This padding is done with the pad_sequencefunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is beam search and why would you use it?\n",
    "A beam search is most often used to maintain tractability in large systems with insufficient amount of memory to store the entire search tree. For example, it has been used in many machine translation systems. (the state of the art now primarily uses neural machine translation based methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is an attention mechanism?\n",
    "Weights of Attention. The weights are used to focus on a particular part of input sequence that is relevant for the token currently being produced by the decoder.\n",
    "Dot-Product Attention. We can compute the relevance or weights by computing the similarity between decoder hidden state to the encoder hidden state.\n",
    "Parameterised Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3771ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "The Transformer architecture excels at handling text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input English sentence to Spanish. At its core, it contains a stack of Encoder layers and Decoder layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e338c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "When would you need to use sampled softmax?\n",
    "For a vector , softmax function is defined as: So, softmax function will do 2 things: 1. convert all scores to probabilities. 2. sum of all probabilities is 1. Recall that in Binary Logistic classifier, we used sigmoid function for the same task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
