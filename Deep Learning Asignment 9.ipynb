{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cae45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are the main tasks that autoencoders are used for?\n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances.\n",
    "Use Naïve Bayes to learn classes from a small labeled data set and then extend it to a large unlabeled data set using the EM (expectation–maximization) iterative clustering algorithm of Section 9.3. The procedure is this. First, train a classifier using the labeled data. Second, apply it to the unlabeled data to label it with class probabilities (the “expectation” step). Third, train a new classifier using the labels for all the data (the “maximization” step). Fourth, iterate until convergence. You could think of this as iterative clustering, where starting points and cluster labels are gleaned from the labeled data. The EM procedure guarantees to find model parameters that have equal or greater likelihood at each iteration. The key question, which can only be answered empirically, is whether these higher likelihood parameter estimates will improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder?\n",
    "If you consider conventional autoencoder function, yes, it is a good autoencoder. In practice, efficiency of autoencoder depends on how well it reconstructs and also on how robust it is to noise in different scenes.\n",
    "\n",
    "Common practice is to add noise sampled from input distribution to the input space to make sure autoencoder, vanilla or VAE, learns to reconstruct the input more robustly regardless of scenic distortions.\n",
    "\n",
    "However, maybe your goal never was reconstruction and thus it doesn’t matter how good reconstruction is. Maybe you wanted to learn features and leverage it for other use. In that case, you wouldn’t care, mostly, about how well reconstruction happens. It is known that noise in input space doesn’t necessary help in better converage of feature space and thus feature learning is hampered. So, community came up with idea of introducing noise in the feature space instead of input space. It will obviously hurt the reconstruction but definitely learned features would be better and overall your feature vector would be more definitive of the latent space as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca013403",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are undercomplete and overcomplete autoencoders?\n",
    "In general, undercomplete autoencoders are used to learn the underly- ing structure of data and used for visualisation/clustering like PCA. In contrast, overcomplete autoencoders are used for classi・…ation based on the assumption that higher dimensionnal features are better for classi・…ation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "How do you tie weights in a stacked autoencoder?\n",
    "In this Autoencoder tutorial, you will learn how to use a stacked autoencoder. The architecture is similar to a traditional neural network. The input goes to a hidden layer in order to be compressed, or reduce its size, and then reaches the reconstruction layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036272de",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is a generative model?\n",
    "Generative models aim to capture the actual distribution of the classes in the dataset.\n",
    "Generative models predict the joint probability distribution – p (x,y) – utilizing Bayes Theorem.\n",
    "Generative models are computationally expensive compared to discriminative models.\n",
    "Generative models are useful for unsupervised machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ddb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is a GAN? Can you name a few tasks where GANs can shine?\n",
    "GANs are an architecture for automatically training a generative model by treating the unsupervised problem as supervised and using both a generative and a discriminative model. GANs provide a path to sophisticated domain-specific data augmentation and a solution to problems that require a generative solution, such as image-to-image translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeac0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are the main difficulties when training GANs?\n",
    "Setting up failure and bad initialization. If you think about it, this is exactly what a GAN is trying to do; the generator and discriminator reach a state where ...\n",
    "Mode collapse. One of the main failure modes with training a generative adversarial network is called mode collapse or sometimes the helvetica scenario.\n",
    "Problem with counting. GANs can sometimes be far-sighted and fail to differentiate the number of particular objects that should occur at a location.\n",
    "Problems with perspective\n",
    "Problems with global structures. GANs do not understand a holistic structure similar to problems with perspective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
